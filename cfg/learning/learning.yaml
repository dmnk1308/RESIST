model:
  _target_: model.models.AttentionModel
  attention_on: sequence
  num_attention_blocks: 5
  num_encoding_functions_electrodes: 10
  num_encoding_functions_points: 10
  pos_encoding: true
  attention_dim: 256
  dropout_attention: 0.2
  num_electrodes: 16
  num_linear_output_blocks: 3
  linear_output_channels: 256
  use_tissue_embedding: false
  use_tissue_body_only: false
  emb_dropout: 0.0
  signal_emb: 8
  prob_dropout: 0.1
training:
  epochs: 200
  batch_size_train: 4
  learning_rate: 5.0e-05
  loss_lung_multiplier: 8
  sample_points: 100000
  device: cuda
validation:
  batch_size_val: 1
  downsample_factor_val: 2
testing:
  batch_size_test: 1
  downsample_factor_test: 2