{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3242])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_convs):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        layers = []\n",
    "        for _ in range(num_convs):\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(nn.ELU())\n",
    "            in_channels = out_channels\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class RBFNet(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_centers):\n",
    "        super(RBFNet, self).__init__()\n",
    "        self.num_centers = num_centers\n",
    "        \n",
    "        # Define the weights, centers, and bandwidths as trainable parameters\n",
    "        self.centers = nn.Parameter(torch.randn(num_centers, in_features))\n",
    "        self.bandwidths = nn.Parameter(torch.ones(num_centers))\n",
    "        self.weights = nn.Linear(num_centers, out_features, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate the squared Euclidean distance between input and centers\n",
    "        expanded_x = x.unsqueeze(1).expand(-1, self.num_centers, -1)  # Shape: (batch_size, num_centers, in_features)\n",
    "        expanded_centers = self.centers.unsqueeze(0).expand(x.size(0), -1, -1)  # Shape: (batch_size, num_centers, in_features)\n",
    "        distances = torch.norm(expanded_x - expanded_centers, dim=2) ** 2  # Shape: (batch_size, num_centers)\n",
    "        \n",
    "        # Calculate the RBF activation\n",
    "        rbf_activations = torch.exp(-distances / (2 * self.bandwidths ** 2))  # Shape: (batch_size, num_centers)\n",
    "        \n",
    "        # Pass through the weights to get the output\n",
    "        output = self.weights(rbf_activations)\n",
    "        return output\n",
    "\n",
    "\n",
    "class ConvNet2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet2D, self).__init__()\n",
    "        self.encoder1 = ConvBlock(1, 64, num_convs=2)\n",
    "        self.encoder2 = ConvBlock(64, 128, num_convs=2)\n",
    "        self.encoder3 = ConvBlock(128, 256, num_convs=4)\n",
    "        self.encoder4 = ConvBlock(256, 512, num_convs=4)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.rbf_net = RBFNet(512, 3242, num_centers=512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        x1 = self.encoder1(x)\n",
    "        x2 = self.pool(x1)\n",
    "        \n",
    "        x3 = self.encoder2(x2)\n",
    "        x4 = self.pool(x3)\n",
    "        \n",
    "        x5 = self.encoder3(x4)\n",
    "        x6 = self.pool(x5)\n",
    "        \n",
    "        x7 = self.encoder4(x6)\n",
    "\n",
    "        # Global Average Pooling\n",
    "        x8 = self.global_avg_pool(x7)\n",
    "        \n",
    "        # Flatten before feeding to RBF network\n",
    "        x8 = x8.view(x8.size(0), -1)\n",
    "        output = self.rbf_net(x8)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model = ConvNet2D()\n",
    "input_tensor = torch.randn(1, 1, 16, 13)  # Batch size 1, 1 input channel, 256x256 image\n",
    "output = model(input_tensor)\n",
    "print(output.shape)  # Should print torch.Size([1, 3242])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pixels equal to 1: 3242\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.draw import ellipse\n",
    "\n",
    "def create_ellipse_mask(center, axes, height, width):\n",
    "    \"\"\"\n",
    "    Create a binary mask with a single ellipse.\n",
    "    \n",
    "    Parameters:\n",
    "    - center: Tuple (y_center, x_center) - Center of the ellipse\n",
    "    - axes: Tuple (a, b) - Axes lengths of the ellipse\n",
    "    - height: Height of the mask\n",
    "    - width: Width of the mask\n",
    "    \n",
    "    Returns:\n",
    "    - Binary mask with the ellipse\n",
    "    \"\"\"\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    rr, cc = ellipse(center[0], center[1], axes[0], axes[1], shape=mask.shape)\n",
    "    mask[rr, cc] = 1\n",
    "    return mask\n",
    "\n",
    "# Define the size of the mask\n",
    "height, width = 128, 128\n",
    "\n",
    "# Ellipse parameters\n",
    "ellipse1_center = (height // 2, width // 4)\n",
    "ellipse1_axes = (height * 0.225, width * 0.1408)\n",
    "\n",
    "ellipse2_center = (height // 2, width * 3 // 4)\n",
    "ellipse2_axes = (height * 0.225, width * 0.1408)\n",
    "\n",
    "# Create masks for each ellipse\n",
    "mask1 = create_ellipse_mask(ellipse1_center, ellipse1_axes, height, width)\n",
    "mask2 = create_ellipse_mask(ellipse2_center, ellipse2_axes, height, width)\n",
    "\n",
    "# Combine masks\n",
    "combined_mask = np.maximum(mask1, mask2)\n",
    "\n",
    "# Count the number of pixels set to 1\n",
    "num_ones = np.sum(combined_mask)\n",
    "\n",
    "# Print the number of pixels with value 1\n",
    "print(f\"Total number of pixels equal to 1: {num_ones}\")\n",
    "\n",
    "# Plot the combined mask\n",
    "plt.imshow(combined_mask, cmap='gray')\n",
    "plt.title(\"Mask with Two Ellipses\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import binary_erosion\n",
    "import fnmatch\n",
    "import torch\n",
    "from train.testing import testing\n",
    "from data_processing.dataset_3d import load_dataset_3d\n",
    "from data_processing.obj2py import read_get, read_mat, read_egt\n",
    "from data_processing.helper import sort_filenames, erode_lung_masks, combine_electrode_positions\n",
    "import copy\n",
    "from plotting_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 128 workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "237it [00:37,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 639, validation set: 96, test set: 120\n"
     ]
    }
   ],
   "source": [
    "eit_path = 'outputs/2024-08-21/16-15-17'\n",
    "model, cfg = load_model(eit_path)#, device='cuda:1')\n",
    "cases = get_all_cases(cfg, base_dir='')    \n",
    "train_dataset, val_dataset, test_dataset = load_dataset_3d(cases,\n",
    "            resolution=cfg.data.resolution, \n",
    "            base_dir = '',\n",
    "            raw_data_folder=cfg.data.raw_data_folder, \n",
    "            processed_data_folder='data/processed/3d_15',\n",
    "            dataset_data_folder=cfg.data.dataset_data_folder,\n",
    "            name_prefix=cfg.data.name_prefix,\n",
    "            write_dataset=True, write_npz=True, \n",
    "            overwrite_npz=False, n_sample_points=cfg.learning.training.sample_points,\n",
    "            apply_rotation=False,\n",
    "            apply_subsampling=False,\n",
    "            apply_translation = False,\n",
    "            translation_x=cfg.data.translation_x, translation_y=cfg.data.translation_y, translation_z=cfg.data.translation_z,\n",
    "            point_levels_3d=4,\n",
    "            multi_process=cfg.data.multi_process, num_workers=cfg.data.num_workers,\n",
    "            signal_norm='all',\n",
    "            normalize_space='xy',\n",
    "            include_resistivities=[5, 15, 10, 20]\n",
    "            )\n",
    "test_dataset.case_files = sort_filenames(test_dataset.case_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3932160, 1])\n"
     ]
    }
   ],
   "source": [
    "for points, signals, electrodes, mask, targets, tissue in train_dataset:\n",
    "    print(targets.reshape())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/tmp/nibdombe/ipykernel_1110/3738553765.py:1: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  targets[mask.reshape(-1)].mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.2000)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[mask.reshape(-1)].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from utils.helper import log_heatmaps, make_cmap\n",
    "\n",
    "def training(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    epochs,\n",
    "    batch_size_train,\n",
    "    lr,\n",
    "    device,\n",
    "    loss_lung_multiplier=1,\n",
    "    batch_size_val=2,\n",
    "    point_levels_3d=9,\n",
    "    output_dir=None,\n",
    "):\n",
    "    print(\"Initializing Dataloader...\", end=\" \")\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size_train, shuffle=True\n",
    "    )\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size_val, shuffle=False)\n",
    "    print(\"Done.\")\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "    loss = nn.MSELoss(reduction=\"none\")\n",
    "    pbar = tqdm(range(epochs))\n",
    "    best_val_loss = 100\n",
    "    cmap = make_cmap()\n",
    "    for epoch in pbar:\n",
    "        epoch_loss_train, epoch_lung_loss_train, model, optimizer = training_step(\n",
    "            model,\n",
    "            train_dataloader,\n",
    "            optimizer,\n",
    "            lr,\n",
    "            loss,\n",
    "            device,\n",
    "            epoch,\n",
    "            loss_lung_multiplier,\n",
    "        )\n",
    "        scheduler.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        if epoch % 1 == 0:  # and epoch!=0:\n",
    "            epoch_loss_val = (\n",
    "                validation_step(\n",
    "                    model,\n",
    "                    val_dataloader,\n",
    "                    loss,\n",
    "                    device,\n",
    "                    cmap,\n",
    "                    point_levels_3d=point_levels_3d,\n",
    "                    loss_lung_multiplier=loss_lung_multiplier,\n",
    "                )\n",
    "            )\n",
    "        pbar.set_description(\n",
    "            f\"Epoch: {epoch+1}, Train Loss: {epoch_loss_train:.5f}, Val Loss: {epoch_loss_val:.5f}\"\n",
    "        )\n",
    "        if epoch_loss_val < best_val_loss:\n",
    "            best_val_loss = epoch_loss_val\n",
    "            checkpoint = {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"loss\": loss,\n",
    "            }\n",
    "            # torch.save(checkpoint, os.path.join(output_dir, \"model.pt\"))\n",
    "        # wandb.log(\n",
    "        #     {\n",
    "        #         \"train_loss\": epoch_loss_train,\n",
    "        #         \"val_loss\": epoch_loss_val,\n",
    "        #         \"best_val_loss\": best_val_loss,\n",
    "        #     }\n",
    "        # )\n",
    "    return model\n",
    "\n",
    "\n",
    "def training_step(\n",
    "    model, dataloader, optimizer, lr, loss, device, epoch, loss_lung_multiplier\n",
    "):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    epoch_loss = 0\n",
    "    lung_loss = 0\n",
    "    for i, (points, signals, electrodes, mask, targets, tissue) in enumerate(\n",
    "        dataloader\n",
    "    ):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        signals = signals.reshape(-1, 1, 16, 13)\n",
    "        \n",
    "        pred = model(\n",
    "            signals.to(device)\n",
    "        )\n",
    "        mask = mask.reshape(targets.shape[0], -1)\n",
    "        targets = targets.reshape(targets.shape[0], -1)\n",
    "        cond =[]\n",
    "        for i in range(targets.shape[0]):\n",
    "            cond.append(targets[i][mask[i]==1].mean())\n",
    "        cond = torch.tensor(cond)\n",
    "        cond = torch.tile(cond, (4,1)).to(device).reshape(-1,1)\n",
    "        targets = torch.ones_like(pred).to(device) * cond\n",
    "        l = loss(pred, targets)\n",
    "        l = l.mean()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += l.detach().cpu()\n",
    "    epoch_loss /= i + 1\n",
    "    lung_loss /= i + 1\n",
    "    return epoch_loss, lung_loss, model, optimizer\n",
    "\n",
    "\n",
    "def validation_step(\n",
    "    model,\n",
    "    dataloader,\n",
    "    loss,\n",
    "    device,\n",
    "    cmap,\n",
    "    point_levels_3d=6,\n",
    "    n_examples=4,\n",
    "    loss_lung_multiplier=1,\n",
    "):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    epoch_loss = 0\n",
    "    for i, (points, signals, electrodes, mask, targets, tissue) in enumerate(\n",
    "        dataloader\n",
    "    ):\n",
    "        \n",
    "        signals = signals.reshape(-1, 1, 16, 13)\n",
    "        \n",
    "        pred = model(\n",
    "            signals.to(device)\n",
    "        )\n",
    "        mask = mask.reshape(targets.shape[0], -1)\n",
    "        targets = targets.reshape(targets.shape[0], -1)\n",
    "        cond =[]\n",
    "        for i in range(targets.shape[0]):\n",
    "            cond.append(targets[i][mask[i]==1].mean())\n",
    "        cond = torch.tensor(cond)\n",
    "        cond = torch.tile(cond, (4,1)).to(device).reshape(-1,1)\n",
    "        targets = torch.ones_like(pred).to(device) * cond\n",
    "        l = loss(pred, targets)\n",
    "        l = l.mean()\n",
    "        epoch_loss += l.detach().cpu()\n",
    "    epoch_loss /= i + 1\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Dataloader... Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train Loss: 0.31813, Val Loss: 0.34167: 100%|██████████| 10/10 [46:03<00:00, 276.34s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConvNet2D(\n",
       "  (encoder1): ConvBlock(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ELU(alpha=1.0)\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ELU(alpha=1.0)\n",
       "    )\n",
       "  )\n",
       "  (encoder2): ConvBlock(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ELU(alpha=1.0)\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ELU(alpha=1.0)\n",
       "    )\n",
       "  )\n",
       "  (encoder3): ConvBlock(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ELU(alpha=1.0)\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ELU(alpha=1.0)\n",
       "      (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ELU(alpha=1.0)\n",
       "      (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ELU(alpha=1.0)\n",
       "    )\n",
       "  )\n",
       "  (encoder4): ConvBlock(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ELU(alpha=1.0)\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ELU(alpha=1.0)\n",
       "      (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ELU(alpha=1.0)\n",
       "      (9): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ELU(alpha=1.0)\n",
       "    )\n",
       "  )\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (rbf_net): RBFNet(\n",
       "    (weights): Linear(in_features=512, out_features=3242, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvNet2D()\n",
    "training(model,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    epochs=10,\n",
    "    batch_size_train=12,\n",
    "    lr=0.001,\n",
    "    device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/tmp/nibdombe/ipykernel_1110/713906156.py:1: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  targets[mask.reshape(-1)].mean(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.2000)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[mask.reshape(-1)].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
